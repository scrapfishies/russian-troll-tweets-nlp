{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian Troll Tweets: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Ideas** \n",
    "- want to see if rate of tweets accelerates as the election comes up\n",
    "    - bar graph of # tweets per month in this dataset\n",
    "    - same as above but for a single user (maybe plot 3-5 together?)\n",
    "- same idea as above but for use of hashtags \n",
    "    - what is the rate of their use? \n",
    "    - a racing bar chart would be cool here (see [Lucy's resource](https://observablehq.com/@d3/gallery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:36.925578Z",
     "start_time": "2020-10-31T02:53:35.608810Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import pickle \n",
    "\n",
    "# visualizations\n",
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, MWETokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "\n",
    "from preprocessing_funcs import clean_tweet, get_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:26:56.978099Z",
     "start_time": "2020-10-30T23:26:56.874773Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:42:53.420740Z",
     "start_time": "2020-10-31T02:42:53.382789Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"words\")\n",
    "# nltk.download(\"europarl_raw\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:39.858409Z",
     "start_time": "2020-10-31T02:53:38.831524Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('../data_files/tweets.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:40.269293Z",
     "start_time": "2020-10-31T02:53:40.250089Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_key</th>\n",
       "      <th>created_at</th>\n",
       "      <th>created_str</th>\n",
       "      <th>retweet_count</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>text</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>source</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>expanded_urls</th>\n",
       "      <th>posted</th>\n",
       "      <th>mentions</th>\n",
       "      <th>retweeted_status_id</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.868981e+09</td>\n",
       "      <td>ryanmaxwell_1</td>\n",
       "      <td>1.458672e+12</td>\n",
       "      <td>2016-03-22 18:31:42</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>7.123460e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"IslamKills\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2.571870e+09</td>\n",
       "      <td>detroitdailynew</td>\n",
       "      <td>1.476133e+12</td>\n",
       "      <td>2016-10-10 20:57:00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>False</td>\n",
       "      <td>0.0</td>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>7.855849e+17</td>\n",
       "      <td>&lt;a href=\"http://twitterfeed.com\" rel=\"nofollow...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[\"http://detne.ws/2e172jF\"]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.710805e+09</td>\n",
       "      <td>cookncooks</td>\n",
       "      <td>1.487767e+12</td>\n",
       "      <td>2017-02-22 12:43:43</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>8.343832e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.584153e+09</td>\n",
       "      <td>queenofthewo</td>\n",
       "      <td>1.482765e+12</td>\n",
       "      <td>2016-12-26 15:06:41</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>8.134006e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[\"ChristmasAftermath\"]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.768260e+09</td>\n",
       "      <td>mrclydepratt</td>\n",
       "      <td>1.501987e+12</td>\n",
       "      <td>2017-08-06 02:36:24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>8.940243e+17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>POSTED</td>\n",
       "      <td>[]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id         user_key    created_at          created_str  \\\n",
       "0  1.868981e+09    ryanmaxwell_1  1.458672e+12  2016-03-22 18:31:42   \n",
       "1  2.571870e+09  detroitdailynew  1.476133e+12  2016-10-10 20:57:00   \n",
       "2  1.710805e+09       cookncooks  1.487767e+12  2017-02-22 12:43:43   \n",
       "3  2.584153e+09     queenofthewo  1.482765e+12  2016-12-26 15:06:41   \n",
       "4  1.768260e+09     mrclydepratt  1.501987e+12  2017-08-06 02:36:24   \n",
       "\n",
       "   retweet_count retweeted  favorite_count  \\\n",
       "0            NaN       NaN             NaN   \n",
       "1            0.0     False             0.0   \n",
       "2            NaN       NaN             NaN   \n",
       "3            NaN       NaN             NaN   \n",
       "4            NaN       NaN             NaN   \n",
       "\n",
       "                                                text      tweet_id  \\\n",
       "0  #IslamKills Are you trying to say that there w...  7.123460e+17   \n",
       "1  Clinton: Trump should’ve apologized more, atta...  7.855849e+17   \n",
       "2  RT @ltapoll: Who was/is the best president of ...  8.343832e+17   \n",
       "3  RT @jww372: I don't have to guess your religio...  8.134006e+17   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...  8.940243e+17   \n",
       "\n",
       "                                              source                hashtags  \\\n",
       "0                                                NaN          [\"IslamKills\"]   \n",
       "1  <a href=\"http://twitterfeed.com\" rel=\"nofollow...                      []   \n",
       "2                                                NaN                      []   \n",
       "3                                                NaN  [\"ChristmasAftermath\"]   \n",
       "4                                                NaN                      []   \n",
       "\n",
       "                 expanded_urls  posted mentions  retweeted_status_id  \\\n",
       "0                           []  POSTED       []                  NaN   \n",
       "1  [\"http://detne.ws/2e172jF\"]  POSTED       []                  NaN   \n",
       "2                           []  POSTED       []                  NaN   \n",
       "3                           []  POSTED       []                  NaN   \n",
       "4                           []  POSTED       []                  NaN   \n",
       "\n",
       "   in_reply_to_status_id  \n",
       "0                    NaN  \n",
       "1                    NaN  \n",
       "2                    NaN  \n",
       "3                    NaN  \n",
       "4                    NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:42.348353Z",
     "start_time": "2020-10-31T02:53:42.260247Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203482 entries, 0 to 203481\n",
      "Data columns (total 16 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   user_id                195417 non-null  float64\n",
      " 1   user_key               203482 non-null  object \n",
      " 2   created_at             203461 non-null  float64\n",
      " 3   created_str            203461 non-null  object \n",
      " 4   retweet_count          58083 non-null   float64\n",
      " 5   retweeted              58083 non-null   object \n",
      " 6   favorite_count         58083 non-null   float64\n",
      " 7   text                   203461 non-null  object \n",
      " 8   tweet_id               201168 non-null  float64\n",
      " 9   source                 58084 non-null   object \n",
      " 10  hashtags               203482 non-null  object \n",
      " 11  expanded_urls          203482 non-null  object \n",
      " 12  posted                 203482 non-null  object \n",
      " 13  mentions               203482 non-null  object \n",
      " 14  retweeted_status_id    39651 non-null   float64\n",
      " 15  in_reply_to_status_id  559 non-null     float64\n",
      "dtypes: float64(7), object(9)\n",
      "memory usage: 24.8+ MB\n"
     ]
    }
   ],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:44.354399Z",
     "start_time": "2020-10-31T02:53:44.201009Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data date range: 2014-07-14 to 2017-09-26\n",
      "Time period: 3.21 years\n"
     ]
    }
   ],
   "source": [
    "# Get date information\n",
    "raw_df['date'] = pd.to_datetime(raw_df['created_str']).dt.date\n",
    "\n",
    "start_date = min(raw_df.date)\n",
    "end_date = max(raw_df.date)\n",
    "time_delta_years = (max(raw_df.date) - min(raw_df.date)).days / 365\n",
    "\n",
    "print(f'Data date range: {start_date} to {end_date}')\n",
    "print(f'Time period: {time_delta_years:.2f} years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T02:53:45.393227Z",
     "start_time": "2020-10-31T02:53:45.377214Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique users: 454\n",
      "Number of Tweets: 203482\n",
      "\n",
      "Average Tweets per User: 448.20\n",
      "Average Tweet per User per Day: 0.38\n",
      "Average Tweet per User per Week: 2.69\n",
      "Average Tweet per User per Month: 11.65\n",
      "Average Tweet per User per Year: 139.82\n"
     ]
    }
   ],
   "source": [
    "num_users = len(raw_df.user_key.unique())\n",
    "num_tweets = len(raw_df)\n",
    "\n",
    "print(f'Number of unique users: {num_users}')\n",
    "print(f'Number of Tweets: {num_tweets}\\n')\n",
    "print(f'Average Tweets per User: {num_tweets/num_users:.2f}')\n",
    "print(f'Average Tweet per User per Day: {(num_tweets/num_users)/(time_delta_years*365):.2f}')\n",
    "print(f'Average Tweet per User per Week: {(num_tweets/num_users)/(time_delta_years*52):.2f}')\n",
    "print(f'Average Tweet per User per Month: {(num_tweets/num_users)/(time_delta_years*12):.2f}')\n",
    "print(f'Average Tweet per User per Year: {(num_tweets/num_users)/(time_delta_years):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just *CLEAN* Tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:47.504127Z",
     "start_time": "2020-10-31T03:09:47.461336Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = raw_df[['text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:48.995301Z",
     "start_time": "2020-10-31T03:09:48.989448Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  #IslamKills Are you trying to say that there w...\n",
       "1  Clinton: Trump should’ve apologized more, atta...\n",
       "2  RT @ltapoll: Who was/is the best president of ...\n",
       "3  RT @jww372: I don't have to guess your religio...\n",
       "4  RT @Shareblue: Pence and his lawyers decided w..."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:54:57.408924Z",
     "start_time": "2020-10-30T16:54:56.407153Z"
    }
   },
   "outputs": [],
   "source": [
    "#df['hashtags'] = df['text'].map(get_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:56.629074Z",
     "start_time": "2020-10-31T03:09:50.739529Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean'] = df['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:57.475021Z",
     "start_time": "2020-10-31T03:09:57.468830Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \n",
       "0  islamkills are you trying to say that there we...  \n",
       "1  clinton trump should ve apologized more attack...  \n",
       "2  who was is the best president of the past year...  \n",
       "3  i don t have to guess your religion christmasa...  \n",
       "4  pence and his lawyers decided which of his off...  "
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove German Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:58.425644Z",
     "start_time": "2020-10-31T03:09:58.299223Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(236736, list)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a list of all english words known to nltk\n",
    "english_words = list(nltk.corpus.words.words())\n",
    "english_words = [word.lower() for word in english_words]\n",
    "len(english_words), type(english_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:59.280362Z",
     "start_time": "2020-10-31T03:09:59.250174Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(147306, list)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_net = list(nltk.corpus.wordnet.words())\n",
    "word_net = [word.lower() for word in word_net]\n",
    "len(word_net), type(word_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:00.261027Z",
     "start_time": "2020-10-31T03:10:00.172157Z"
    }
   },
   "outputs": [],
   "source": [
    "many_words = set(english_words + word_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:01.883025Z",
     "start_time": "2020-10-31T03:10:01.879744Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(323592, set)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(many_words), type(many_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:04.139306Z",
     "start_time": "2020-10-31T03:10:04.136335Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'email' in many_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:05.911395Z",
     "start_time": "2020-10-31T03:10:05.900713Z"
    }
   },
   "outputs": [],
   "source": [
    "from nltk.corpus.europarl_raw import german"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:07.742658Z",
     "start_time": "2020-10-31T03:10:07.265502Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(556226, list)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "german = list(german.words())\n",
    "german = [word.lower() for word in german]\n",
    "len(german), type(german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:10.142079Z",
     "start_time": "2020-10-31T03:10:10.037593Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_german = set([word for word in german if word not in many_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:11.980611Z",
     "start_time": "2020-10-31T03:10:11.977428Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28154"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(remove_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:09:27.870687Z",
     "start_time": "2020-10-31T03:09:27.868071Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_german_words = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w not in remove_german)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:34.380532Z",
     "start_time": "2020-10-31T03:10:17.724531Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop non-english words\n",
    "df['clean'] = df['clean'].apply(remove_german_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:35.282372Z",
     "start_time": "2020-10-31T03:10:35.241365Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 203482 entries, 0 to 203481\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count   Dtype \n",
      "---  ------  --------------   ----- \n",
      " 0   text    203482 non-null  object\n",
      " 1   clean   203482 non-null  object\n",
      "dtypes: object(2)\n",
      "memory usage: 3.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:38.550749Z",
     "start_time": "2020-10-31T03:10:38.543555Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \n",
       "0  islamkills are you trying to say that there we...  \n",
       "1  clinton trump should ve apologized more attack...  \n",
       "2  who was is the best president of the past year...  \n",
       "3  i don t have to guess your religion christmasa...  \n",
       "4  pence and his lawyers decided which of his off...  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:55.812045Z",
     "start_time": "2020-10-31T03:10:55.807619Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(list, 179)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_stop_words = stopwords.words(\"english\")\n",
    "print(standard_stop_words)\n",
    "type(standard_stop_words), len(standard_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:10:57.050051Z",
     "start_time": "2020-10-31T03:10:57.046040Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data_files/twitter_stopwords.txt') as f:\n",
    "    words = f.read().split(',')\n",
    "    twitter_stopwords = list(words)\n",
    "\n",
    "len(twitter_stopwords)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:20.320645Z",
     "start_time": "2020-10-31T03:11:20.318149Z"
    }
   },
   "outputs": [],
   "source": [
    "twitter_stopwords = [word.lower() for word in twitter_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:22.242572Z",
     "start_time": "2020-10-31T03:11:22.240367Z"
    }
   },
   "outputs": [],
   "source": [
    "other_words = ['amp', '…']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:35.186657Z",
     "start_time": "2020-10-31T03:11:35.184284Z"
    }
   },
   "outputs": [],
   "source": [
    "all_the_stops = set(standard_stop_words + twitter_stopwords + other_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:36.937983Z",
     "start_time": "2020-10-31T03:11:36.934863Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_the_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:39.637592Z",
     "start_time": "2020-10-31T03:11:39.635030Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_stop_words = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w not in all_the_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:11:59.840390Z",
     "start_time": "2020-10-31T03:11:43.195317Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop stop-words\n",
    "df['no_stops'] = df['clean'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:12:09.508422Z",
     "start_time": "2020-10-31T03:12:09.505398Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = '''!’()-![]{};:+'\"\"\\,<>./?@#$%^&*_~'''                                   \n",
    "no_punc = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w not in punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:12:28.568994Z",
     "start_time": "2020-10-31T03:12:13.155907Z"
    }
   },
   "outputs": [],
   "source": [
    "df['no_stops'] = df['no_stops'].apply(no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:12:29.442860Z",
     "start_time": "2020-10-31T03:12:29.435544Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "      <td>islamkills trying say terrorist attacks europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "      <td>clinton trump apologized attacked less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "      <td>best president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "      <td>pence lawyers decided official emails public c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  islamkills are you trying to say that there we...   \n",
       "1  clinton trump should ve apologized more attack...   \n",
       "2  who was is the best president of the past year...   \n",
       "3  i don t have to guess your religion christmasa...   \n",
       "4  pence and his lawyers decided which of his off...   \n",
       "\n",
       "                                            no_stops  \n",
       "0  islamkills trying say terrorist attacks europe...  \n",
       "1             clinton trump apologized attacked less  \n",
       "2                        best president past retweet  \n",
       "3                  guess religion christmasaftermath  \n",
       "4  pence lawyers decided official emails public c...  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:12:39.713774Z",
     "start_time": "2020-10-31T03:12:39.108678Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:12:40.686180Z",
     "start_time": "2020-10-31T03:12:40.683687Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_lemmatize = lambda x: \" \".join([token.lemma_ for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:07.569125Z",
     "start_time": "2020-10-31T03:12:44.686314Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"lem\"] = df[\"no_stops\"].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:08.420696Z",
     "start_time": "2020-10-31T03:17:08.412998Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>no_stops</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "      <td>islamkills trying say terrorist attacks europe...</td>\n",
       "      <td>islamkill try say terrorist attack europe refu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "      <td>clinton trump apologized attacked less</td>\n",
       "      <td>clinton trump apologize attack less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "      <td>best president past retweet</td>\n",
       "      <td>good president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "      <td>pence lawyers decided official emails public c...</td>\n",
       "      <td>pence lawyer decide official email public coul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  islamkills are you trying to say that there we...   \n",
       "1  clinton trump should ve apologized more attack...   \n",
       "2  who was is the best president of the past year...   \n",
       "3  i don t have to guess your religion christmasa...   \n",
       "4  pence and his lawyers decided which of his off...   \n",
       "\n",
       "                                            no_stops  \\\n",
       "0  islamkills trying say terrorist attacks europe...   \n",
       "1             clinton trump apologized attacked less   \n",
       "2                        best president past retweet   \n",
       "3                  guess religion christmasaftermath   \n",
       "4  pence lawyers decided official emails public c...   \n",
       "\n",
       "                                                 lem  \n",
       "0  islamkill try say terrorist attack europe refu...  \n",
       "1                clinton trump apologize attack less  \n",
       "2                        good president past retweet  \n",
       "3                  guess religion christmasaftermath  \n",
       "4  pence lawyer decide official email public coul...  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:09.261566Z",
     "start_time": "2020-10-31T03:17:09.258500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IslamKills Are you trying to say that there were no terrorist attacks in Europe before refugees were let in?\n",
      "islamkill try say terrorist attack europe refugee let \n",
      "\n",
      "Clinton: Trump should’ve apologized more, attacked less https://t.co/eJampkoHFZ\n",
      "clinton trump apologize attack less \n",
      "\n",
      "RT @ltapoll: Who was/is the best president of the past 25 years? (Vote &amp; Retweet)\n",
      "good president past retweet \n",
      "\n",
      "RT @jww372: I don't have to guess your religion! #ChristmasAftermath\n",
      "guess religion christmasaftermath \n",
      "\n",
      "RT @Shareblue: Pence and his lawyers decided which of his official emails the public could see\r\n",
      "\r\n",
      "https://t.co/HjhPguBK1Y by @alisonrose711\n",
      "pence lawyer decide official email public could see \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(df.text[i])\n",
    "    print(df.lem[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:10.716164Z",
     "start_time": "2020-10-31T03:17:10.107880Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 30887),\n",
       " ('clinton', 13015),\n",
       " ('hillary', 11780),\n",
       " ('obama', 9790),\n",
       " ('get', 8862),\n",
       " ('say', 8193),\n",
       " ('people', 6845),\n",
       " ('go', 6262),\n",
       " ('make', 6113),\n",
       " ('like', 5796)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = []\n",
    "for tweet in list(df['lem']):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "Counter(all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:29.189329Z",
     "start_time": "2020-10-31T03:17:23.404239Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnndddddddddddddddddddddddddddddddddd</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddd</th>\n",
       "      <th>aaaaaaaaannnnnnnnnnnddddddddddddd</th>\n",
       "      <th>aaaaaaaaassssss</th>\n",
       "      <th>aaaaaand</th>\n",
       "      <th>aaaaaayyyuuuummmmmm</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>...</th>\n",
       "      <th>zynalturist</th>\n",
       "      <th>zynischen</th>\n",
       "      <th>zyoritv</th>\n",
       "      <th>zzcrane</th>\n",
       "      <th>zzion</th>\n",
       "      <th>zzjwmc</th>\n",
       "      <th>zzzs</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203482 rows × 83709 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  \\\n",
       "0        0    0   \n",
       "1        0    0   \n",
       "2        0    0   \n",
       "3        0    0   \n",
       "4        0    0   \n",
       "...     ..  ...   \n",
       "203477   0    0   \n",
       "203478   0    0   \n",
       "203479   0    0   \n",
       "203480   0    0   \n",
       "203481   0    0   \n",
       "\n",
       "        aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnndddddddddddddddddddddddddddddddddd  \\\n",
       "0                                                       0                                                                    \n",
       "1                                                       0                                                                    \n",
       "2                                                       0                                                                    \n",
       "3                                                       0                                                                    \n",
       "4                                                       0                                                                    \n",
       "...                                                   ...                                                                    \n",
       "203477                                                  0                                                                    \n",
       "203478                                                  0                                                                    \n",
       "203479                                                  0                                                                    \n",
       "203480                                                  0                                                                    \n",
       "203481                                                  0                                                                    \n",
       "\n",
       "        aaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddd  \\\n",
       "0                                                       0                          \n",
       "1                                                       0                          \n",
       "2                                                       0                          \n",
       "3                                                       0                          \n",
       "4                                                       0                          \n",
       "...                                                   ...                          \n",
       "203477                                                  0                          \n",
       "203478                                                  0                          \n",
       "203479                                                  0                          \n",
       "203480                                                  0                          \n",
       "203481                                                  0                          \n",
       "\n",
       "        aaaaaaaaannnnnnnnnnnddddddddddddd  aaaaaaaaassssss  aaaaaand  \\\n",
       "0                                       0                0         0   \n",
       "1                                       0                0         0   \n",
       "2                                       0                0         0   \n",
       "3                                       0                0         0   \n",
       "4                                       0                0         0   \n",
       "...                                   ...              ...       ...   \n",
       "203477                                  0                0         0   \n",
       "203478                                  0                0         0   \n",
       "203479                                  0                0         0   \n",
       "203480                                  0                0         0   \n",
       "203481                                  0                0         0   \n",
       "\n",
       "        aaaaaayyyuuuummmmmm  aaaaand  aaaand  ...  zynalturist  zynischen  \\\n",
       "0                         0        0       0  ...            0          0   \n",
       "1                         0        0       0  ...            0          0   \n",
       "2                         0        0       0  ...            0          0   \n",
       "3                         0        0       0  ...            0          0   \n",
       "4                         0        0       0  ...            0          0   \n",
       "...                     ...      ...     ...  ...          ...        ...   \n",
       "203477                    0        0       0  ...            0          0   \n",
       "203478                    0        0       0  ...            0          0   \n",
       "203479                    0        0       0  ...            0          0   \n",
       "203480                    0        0       0  ...            0          0   \n",
       "203481                    0        0       0  ...            0          0   \n",
       "\n",
       "        zyoritv  zzcrane  zzion  zzjwmc  zzzs  zzzzzz  zzzzzzz  \\\n",
       "0             0        0      0       0     0       0        0   \n",
       "1             0        0      0       0     0       0        0   \n",
       "2             0        0      0       0     0       0        0   \n",
       "3             0        0      0       0     0       0        0   \n",
       "4             0        0      0       0     0       0        0   \n",
       "...         ...      ...    ...     ...   ...     ...      ...   \n",
       "203477        0        0      0       0     0       0        0   \n",
       "203478        0        0      0       0     0       0        0   \n",
       "203479        0        0      0       0     0       0        0   \n",
       "203480        0        0      0       0     0       0        0   \n",
       "203481        0        0      0       0     0       0        0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzz  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "203477                  0  \n",
       "203478                  0  \n",
       "203479                  0  \n",
       "203480                  0  \n",
       "203481                  0  \n",
       "\n",
       "[203482 rows x 83709 columns]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "doc_word = cv.fit_transform(df[\"lem\"])\n",
    "vect = pd.DataFrame(doc_word.toarray(),columns=cv.get_feature_names())\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:58.801010Z",
     "start_time": "2020-10-31T03:17:58.796781Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zushaelinson', 'zust', 'zuwachs', 'zuwanderer', 'zuzu', 'zuzuceo',\n",
       "       'zvezdanews', 'zvjezdanpatz', 'zw', 'zwangsfinanzierte', 'zwangsr',\n",
       "       'zweifach', 'zweifaktor', 'zweij', 'zweitwichtigste', 'zwilling',\n",
       "       'zwillingen', 'zwischendurch', 'zwischenstationen', 'zyka',\n",
       "       'zynalturist', 'zynischen', 'zyoritv', 'zzcrane', 'zzion', 'zzjwmc',\n",
       "       'zzzs', 'zzzzzz', 'zzzzzzz', 'zzzzzzzzzzzzzzzzz'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.columns[-30:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:17:37.713978Z",
     "start_time": "2020-10-31T03:17:37.710048Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11100"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previously: 94809 columns\n",
    "# spacy lem: 89679\n",
    "94809-83709"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:19:04.046086Z",
     "start_time": "2020-10-31T03:19:04.030599Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkill try say terrorist attack europe refu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump apologize attack less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>good president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence lawyer decide official email public coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203477</th>\n",
       "      <td>RT @AndreaChalupa: In intel circles, the story...</td>\n",
       "      <td>intel circles story go fsb film trump orgy rus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203478</th>\n",
       "      <td>RT @KansasCityDNews: Tonganoxie police: Middle...</td>\n",
       "      <td>tonganoxie police middle school girl sexually ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203479</th>\n",
       "      <td>RT @signsinyork: Getting the right #company lo...</td>\n",
       "      <td>get right company logo business get message ac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203480</th>\n",
       "      <td>The Latest: Obama affirms continuity of ties w...</td>\n",
       "      <td>late obama affirm continuity tie canada</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203481</th>\n",
       "      <td>RT @futureguru100: U cant just Upload a CD onl...</td>\n",
       "      <td>u can not upload cd online that s product work...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203482 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       #IslamKills Are you trying to say that there w...   \n",
       "1       Clinton: Trump should’ve apologized more, atta...   \n",
       "2       RT @ltapoll: Who was/is the best president of ...   \n",
       "3       RT @jww372: I don't have to guess your religio...   \n",
       "4       RT @Shareblue: Pence and his lawyers decided w...   \n",
       "...                                                   ...   \n",
       "203477  RT @AndreaChalupa: In intel circles, the story...   \n",
       "203478  RT @KansasCityDNews: Tonganoxie police: Middle...   \n",
       "203479  RT @signsinyork: Getting the right #company lo...   \n",
       "203480  The Latest: Obama affirms continuity of ties w...   \n",
       "203481  RT @futureguru100: U cant just Upload a CD onl...   \n",
       "\n",
       "                                                      lem  \n",
       "0       islamkill try say terrorist attack europe refu...  \n",
       "1                     clinton trump apologize attack less  \n",
       "2                             good president past retweet  \n",
       "3                       guess religion christmasaftermath  \n",
       "4       pence lawyer decide official email public coul...  \n",
       "...                                                   ...  \n",
       "203477  intel circles story go fsb film trump orgy rus...  \n",
       "203478  tonganoxie police middle school girl sexually ...  \n",
       "203479  get right company logo business get message ac...  \n",
       "203480            late obama affirm continuity tie canada  \n",
       "203481  u can not upload cd online that s product work...  \n",
       "\n",
       "[203482 rows x 2 columns]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df[['text', 'lem']]\n",
    "tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T03:19:11.791445Z",
     "start_time": "2020-10-31T03:19:11.650987Z"
    }
   },
   "outputs": [],
   "source": [
    "# with open('../data_files/tweets.pickle', 'wb') as to_write:\n",
    "#    pickle.dump(tweets, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most frequent tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:55:07.792178Z",
     "start_time": "2020-10-30T16:55:07.308081Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(df['user_key'])\n",
    "\n",
    "wc = WordCloud(width=600, height=400, max_words=50).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most used hashtags in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:55:12.289599Z",
     "start_time": "2020-10-30T16:55:11.187562Z"
    }
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(df['hashtags'].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "wc = WordCloud(width=800, height=600, max_words=150).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merkelmussbleiben = 'Merkel must stay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:28.670814Z",
     "start_time": "2020-10-30T18:50:28.668347Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:29.859732Z",
     "start_time": "2020-10-30T18:50:29.855720Z"
    }
   },
   "outputs": [],
   "source": [
    "class NLPPipe:\n",
    "    \n",
    "    def __init__(self, vectorizer=CountVectorizer(), cleaning_function=None, tokenizer=None, stemmer=None):\n",
    "        '''\n",
    "        Create a pipeline that vectorizes an arbitary list of documents.\n",
    "        '''\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "     \n",
    "    def clean_text(self, text):\n",
    "        text = text.astype(str)\n",
    "        cleaned_text = text.map(self.cleaning_function)\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        pass\n",
    "        \n",
    "    def transform(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:31.850267Z",
     "start_time": "2020-10-30T18:50:31.847578Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = NLPPipe(vectorizer=CountVectorizer(), \n",
    "              cleaning_function=clean_tweet, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:34.586213Z",
     "start_time": "2020-10-30T18:50:34.570937Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = raw_df[['text']]\n",
    "\n",
    "tweets = df2['text']\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:51:26.033091Z",
     "start_time": "2020-10-30T18:51:20.765425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tweets = nlp.clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:53:44.913592Z",
     "start_time": "2020-10-30T18:53:44.907551Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T17:31:05.778478Z",
     "start_time": "2020-10-30T17:31:05.775095Z"
    }
   },
   "outputs": [],
   "source": [
    "for tweet in clean_tweets[0:5]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T17:31:40.518752Z",
     "start_time": "2020-10-30T17:31:40.515670Z"
    }
   },
   "outputs": [],
   "source": [
    "for tweet in raw_df.text[0:5]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data_files/clean_tweets.zip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
