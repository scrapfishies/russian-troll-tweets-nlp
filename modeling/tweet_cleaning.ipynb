{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Russian Troll Tweets: Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EDA Ideas** \n",
    "- want to see if rate of tweets accelerates as the election comes up\n",
    "    - bar graph of # tweets per month in this dataset\n",
    "    - same as above but for a single user (maybe plot 3-5 together?)\n",
    "- same idea as above but for use of hashtags \n",
    "    - what is the rate of their use? \n",
    "    - a racing bar chart would be cool here (see [Lucy's resource](https://observablehq.com/@d3/gallery))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:20:11.753422Z",
     "start_time": "2020-10-31T01:20:11.745235Z"
    }
   },
   "outputs": [],
   "source": [
    "# Basics\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import re\n",
    "import string\n",
    "import datetime\n",
    "import pickle \n",
    "\n",
    "# visualizations\n",
    "from wordcloud import WordCloud\n",
    "from nltk import FreqDist\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize, MWETokenizer\n",
    "from nltk.util import ngrams\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "import spacy\n",
    "\n",
    "from preprocessing_funcs import clean_tweet, get_hashtags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:26:56.978099Z",
     "start_time": "2020-10-30T23:26:56.874773Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:40:19.482388Z",
     "start_time": "2020-10-30T23:40:19.284873Z"
    }
   },
   "outputs": [],
   "source": [
    "# nltk.download(\"punkt\")\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"words\")\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"averaged_perceptron_tagger\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:02.008385Z",
     "start_time": "2020-10-31T01:10:00.993201Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df = pd.read_csv('../data_files/tweets.csv.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:42:32.111108Z",
     "start_time": "2020-10-31T00:42:32.090676Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:42:32.483133Z",
     "start_time": "2020-10-31T00:42:32.395648Z"
    }
   },
   "outputs": [],
   "source": [
    "raw_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:42:34.181862Z",
     "start_time": "2020-10-31T00:42:34.028319Z"
    }
   },
   "outputs": [],
   "source": [
    "# Get date information\n",
    "raw_df['date'] = pd.to_datetime(raw_df['created_str']).dt.date\n",
    "\n",
    "start_date = min(raw_df.date)\n",
    "end_date = max(raw_df.date)\n",
    "time_delta_years = (max(raw_df.date) - min(raw_df.date)).days / 365\n",
    "\n",
    "print(f'Data date range: {start_date} to {end_date}')\n",
    "print(f'Time period: {time_delta_years:.2f} years')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T00:42:36.337281Z",
     "start_time": "2020-10-31T00:42:36.321423Z"
    }
   },
   "outputs": [],
   "source": [
    "num_users = len(raw_df.user_key.unique())\n",
    "num_tweets = len(raw_df)\n",
    "\n",
    "print(f'Number of unique users: {num_users}')\n",
    "print(f'Number of Tweets: {num_tweets}\\n')\n",
    "print(f'Average Tweets per User: {num_tweets/num_users:.2f}')\n",
    "print(f'Average Tweet per User per Day: {(num_tweets/num_users)/(time_delta_years*365):.2f}')\n",
    "print(f'Average Tweet per User per Week: {(num_tweets/num_users)/(time_delta_years*52):.2f}')\n",
    "print(f'Average Tweet per User per Month: {(num_tweets/num_users)/(time_delta_years*12):.2f}')\n",
    "print(f'Average Tweet per User per Year: {(num_tweets/num_users)/(time_delta_years):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Just *CLEAN* Tweets!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:04.044202Z",
     "start_time": "2020-10-31T01:10:04.000833Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df = raw_df[['text']].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:05.046560Z",
     "start_time": "2020-10-31T01:10:05.038323Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  #IslamKills Are you trying to say that there w...\n",
       "1  Clinton: Trump should’ve apologized more, atta...\n",
       "2  RT @ltapoll: Who was/is the best president of ...\n",
       "3  RT @jww372: I don't have to guess your religio...\n",
       "4  RT @Shareblue: Pence and his lawyers decided w..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:54:57.408924Z",
     "start_time": "2020-10-30T16:54:56.407153Z"
    }
   },
   "outputs": [],
   "source": [
    "#df['hashtags'] = df['text'].map(get_hashtags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:12.219799Z",
     "start_time": "2020-10-31T01:10:06.544050Z"
    }
   },
   "outputs": [],
   "source": [
    "df['clean'] = df['text'].map(clean_tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:13.987284Z",
     "start_time": "2020-10-31T01:10:13.980784Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \n",
       "0  islamkills are you trying to say that there we...  \n",
       "1  clinton trump should ve apologized more attack...  \n",
       "2  who was is the best president of the past year...  \n",
       "3  i don t have to guess your religion christmasa...  \n",
       "4  pence and his lawyers decided which of his off...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Remove non-english words\n",
    "\n",
    "This didn't work well -- removed too many important words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:41:16.892201Z",
     "start_time": "2020-10-30T23:41:16.800623Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# a list of all english words known to nltk\n",
    "words = list(nltk.corpus.words.words())\n",
    "len(words), type(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:41:18.287868Z",
     "start_time": "2020-10-30T23:41:18.280829Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "word_net = list(nltk.corpus.wordnet.words())\n",
    "len(word_net), type(word_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:43:57.590509Z",
     "start_time": "2020-10-30T23:43:57.526472Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "many_words = set(words + word_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:44:15.785515Z",
     "start_time": "2020-10-30T23:44:15.782391Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "len(many_words), type(many_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:44:30.782974Z",
     "start_time": "2020-10-30T23:44:30.779821Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'email' in many_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:44:32.931178Z",
     "start_time": "2020-10-30T23:44:32.928618Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "delete_non_english = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w in many_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:44:54.162052Z",
     "start_time": "2020-10-30T23:44:35.576037Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# drop non-english words\n",
    "df['clean'] = df['clean'].apply(delete_non_english)\n",
    "\n",
    "# drop any tweet rows \n",
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:45:51.108138Z",
     "start_time": "2020-10-30T23:45:51.101143Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:47:37.718037Z",
     "start_time": "2020-10-30T23:47:37.714720Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for tweet in df['clean'][5:15]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T23:47:17.170660Z",
     "start_time": "2020-10-30T23:47:17.167502Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for tweet in df['text'][5:15]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Removing stop words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:28.887837Z",
     "start_time": "2020-10-31T01:10:28.883533Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(list, 179)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "standard_stop_words = stopwords.words(\"english\")\n",
    "print(standard_stop_words)\n",
    "type(standard_stop_words), len(standard_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:31.806167Z",
     "start_time": "2020-10-31T01:10:31.801373Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "623"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('../data_files/twitter_stopwords.txt') as f:\n",
    "    words = f.read().split(',')\n",
    "    twitter_stopwords = list(words)\n",
    "\n",
    "len(twitter_stopwords)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:54.389936Z",
     "start_time": "2020-10-31T01:10:54.387590Z"
    }
   },
   "outputs": [],
   "source": [
    "other_words = ['amp', '…']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:10:57.579160Z",
     "start_time": "2020-10-31T01:10:57.576828Z"
    }
   },
   "outputs": [],
   "source": [
    "all_the_stops = set(standard_stop_words + twitter_stopwords + other_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:02.535426Z",
     "start_time": "2020-10-31T01:11:02.532197Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "616"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_the_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:14.919041Z",
     "start_time": "2020-10-31T01:11:14.916535Z"
    }
   },
   "outputs": [],
   "source": [
    "remove_stop_words = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w not in all_the_stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:36.026430Z",
     "start_time": "2020-10-31T01:11:19.047560Z"
    }
   },
   "outputs": [],
   "source": [
    "# drop stop-words\n",
    "df['no_stops'] = df['clean'].apply(remove_stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:36.303560Z",
     "start_time": "2020-10-31T01:11:36.300496Z"
    }
   },
   "outputs": [],
   "source": [
    "punctuations = '''!’()-![]{};:+'\"\"\\,<>./?@#$%^&*_~'''                                   \n",
    "no_punc = lambda x: \" \".join(w for w in nltk.word_tokenize(x) if w not in punctuations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:51.284572Z",
     "start_time": "2020-10-31T01:11:36.589387Z"
    }
   },
   "outputs": [],
   "source": [
    "df['no_stops'] = df['no_stops'].apply(no_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:11:51.560428Z",
     "start_time": "2020-10-31T01:11:51.553792Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>no_stops</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "      <td>islamkills trying say terrorist attacks europe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "      <td>clinton trump apologized attacked less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "      <td>best president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "      <td>pence lawyers decided official emails public c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  islamkills are you trying to say that there we...   \n",
       "1  clinton trump should ve apologized more attack...   \n",
       "2  who was is the best president of the past year...   \n",
       "3  i don t have to guess your religion christmasa...   \n",
       "4  pence and his lawyers decided which of his off...   \n",
       "\n",
       "                                            no_stops  \n",
       "0  islamkills trying say terrorist attacks europe...  \n",
       "1             clinton trump apologized attacked less  \n",
       "2                        best president past retweet  \n",
       "3                  guess religion christmasaftermath  \n",
       "4  pence lawyers decided official emails public c...  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:12:05.515979Z",
     "start_time": "2020-10-31T01:12:04.809118Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[\"parser\", \"ner\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:12:07.788590Z",
     "start_time": "2020-10-31T01:12:07.786202Z"
    }
   },
   "outputs": [],
   "source": [
    "spacy_lemmatize = lambda x: \" \".join([token.lemma_ for token in nlp(x)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:17:17.172571Z",
     "start_time": "2020-10-31T01:12:47.231153Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"lem\"] = df[\"no_stops\"].apply(spacy_lemmatize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:17:17.474996Z",
     "start_time": "2020-10-31T01:17:17.467102Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>clean</th>\n",
       "      <th>no_stops</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkills are you trying to say that there we...</td>\n",
       "      <td>islamkills trying say terrorist attacks europe...</td>\n",
       "      <td>islamkill try say terrorist attack europe refu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump should ve apologized more attack...</td>\n",
       "      <td>clinton trump apologized attacked less</td>\n",
       "      <td>clinton trump apologize attack less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>who was is the best president of the past year...</td>\n",
       "      <td>best president past retweet</td>\n",
       "      <td>good president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>i don t have to guess your religion christmasa...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence and his lawyers decided which of his off...</td>\n",
       "      <td>pence lawyers decided official emails public c...</td>\n",
       "      <td>pence lawyer decide official email public coul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                               clean  \\\n",
       "0  islamkills are you trying to say that there we...   \n",
       "1  clinton trump should ve apologized more attack...   \n",
       "2  who was is the best president of the past year...   \n",
       "3  i don t have to guess your religion christmasa...   \n",
       "4  pence and his lawyers decided which of his off...   \n",
       "\n",
       "                                            no_stops  \\\n",
       "0  islamkills trying say terrorist attacks europe...   \n",
       "1             clinton trump apologized attacked less   \n",
       "2                        best president past retweet   \n",
       "3                  guess religion christmasaftermath   \n",
       "4  pence lawyers decided official emails public c...   \n",
       "\n",
       "                                                 lem  \n",
       "0  islamkill try say terrorist attack europe refu...  \n",
       "1                clinton trump apologize attack less  \n",
       "2                        good president past retweet  \n",
       "3                  guess religion christmasaftermath  \n",
       "4  pence lawyer decide official email public coul...  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:17:17.774977Z",
     "start_time": "2020-10-31T01:17:17.760885Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#IslamKills Are you trying to say that there were no terrorist attacks in Europe before refugees were let in?\n",
      "islamkill try say terrorist attack europe refugee let \n",
      "\n",
      "Clinton: Trump should’ve apologized more, attacked less https://t.co/eJampkoHFZ\n",
      "clinton trump apologize attack less \n",
      "\n",
      "RT @ltapoll: Who was/is the best president of the past 25 years? (Vote &amp; Retweet)\n",
      "good president past retweet \n",
      "\n",
      "RT @jww372: I don't have to guess your religion! #ChristmasAftermath\n",
      "guess religion christmasaftermath \n",
      "\n",
      "RT @Shareblue: Pence and his lawyers decided which of his official emails the public could see\r\n",
      "\r\n",
      "https://t.co/HjhPguBK1Y by @alisonrose711\n",
      "pence lawyer decide official email public could see \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(0, 5):\n",
    "    print(df.text[i])\n",
    "    print(df.lem[i], '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:17:18.642080Z",
     "start_time": "2020-10-31T01:17:18.057451Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('trump', 30886),\n",
       " ('clinton', 13015),\n",
       " ('hillary', 11780),\n",
       " ('obama', 9790),\n",
       " ('get', 8862),\n",
       " ('say', 8193),\n",
       " ('people', 6845),\n",
       " ('go', 6260),\n",
       " ('make', 6113),\n",
       " ('like', 5796)]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_words = []\n",
    "for tweet in list(df['lem']):\n",
    "    words = tweet.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "    \n",
    "Counter(all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:17:59.639358Z",
     "start_time": "2020-10-31T01:17:54.228013Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>aa</th>\n",
       "      <th>aaa</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnndddddddddddddddddddddddddddddddddd</th>\n",
       "      <th>aaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddd</th>\n",
       "      <th>aaaaaaaaannnnnnnnnnnddddddddddddd</th>\n",
       "      <th>aaaaaaaaassssss</th>\n",
       "      <th>aaaaaand</th>\n",
       "      <th>aaaaaayyyuuuummmmmm</th>\n",
       "      <th>aaaaand</th>\n",
       "      <th>aaaand</th>\n",
       "      <th>...</th>\n",
       "      <th>zynalturist</th>\n",
       "      <th>zynischen</th>\n",
       "      <th>zyoritv</th>\n",
       "      <th>zzcrane</th>\n",
       "      <th>zzion</th>\n",
       "      <th>zzjwmc</th>\n",
       "      <th>zzzs</th>\n",
       "      <th>zzzzzz</th>\n",
       "      <th>zzzzzzz</th>\n",
       "      <th>zzzzzzzzzzzzzzzzz</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203477</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203478</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203479</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203480</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>203481</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>203482 rows × 86277 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        aa  aaa  \\\n",
       "0        0    0   \n",
       "1        0    0   \n",
       "2        0    0   \n",
       "3        0    0   \n",
       "4        0    0   \n",
       "...     ..  ...   \n",
       "203477   0    0   \n",
       "203478   0    0   \n",
       "203479   0    0   \n",
       "203480   0    0   \n",
       "203481   0    0   \n",
       "\n",
       "        aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnnnnnnnnnnnndddddddddddddddddddddddddddddddddd  \\\n",
       "0                                                       0                                                                    \n",
       "1                                                       0                                                                    \n",
       "2                                                       0                                                                    \n",
       "3                                                       0                                                                    \n",
       "4                                                       0                                                                    \n",
       "...                                                   ...                                                                    \n",
       "203477                                                  0                                                                    \n",
       "203478                                                  0                                                                    \n",
       "203479                                                  0                                                                    \n",
       "203480                                                  0                                                                    \n",
       "203481                                                  0                                                                    \n",
       "\n",
       "        aaaaaaaaaaaaaaaaaaaaannnnnnnnnnnnnnnnnnnnddddddddddddddddddddddddddddddd  \\\n",
       "0                                                       0                          \n",
       "1                                                       0                          \n",
       "2                                                       0                          \n",
       "3                                                       0                          \n",
       "4                                                       0                          \n",
       "...                                                   ...                          \n",
       "203477                                                  0                          \n",
       "203478                                                  0                          \n",
       "203479                                                  0                          \n",
       "203480                                                  0                          \n",
       "203481                                                  0                          \n",
       "\n",
       "        aaaaaaaaannnnnnnnnnnddddddddddddd  aaaaaaaaassssss  aaaaaand  \\\n",
       "0                                       0                0         0   \n",
       "1                                       0                0         0   \n",
       "2                                       0                0         0   \n",
       "3                                       0                0         0   \n",
       "4                                       0                0         0   \n",
       "...                                   ...              ...       ...   \n",
       "203477                                  0                0         0   \n",
       "203478                                  0                0         0   \n",
       "203479                                  0                0         0   \n",
       "203480                                  0                0         0   \n",
       "203481                                  0                0         0   \n",
       "\n",
       "        aaaaaayyyuuuummmmmm  aaaaand  aaaand  ...  zynalturist  zynischen  \\\n",
       "0                         0        0       0  ...            0          0   \n",
       "1                         0        0       0  ...            0          0   \n",
       "2                         0        0       0  ...            0          0   \n",
       "3                         0        0       0  ...            0          0   \n",
       "4                         0        0       0  ...            0          0   \n",
       "...                     ...      ...     ...  ...          ...        ...   \n",
       "203477                    0        0       0  ...            0          0   \n",
       "203478                    0        0       0  ...            0          0   \n",
       "203479                    0        0       0  ...            0          0   \n",
       "203480                    0        0       0  ...            0          0   \n",
       "203481                    0        0       0  ...            0          0   \n",
       "\n",
       "        zyoritv  zzcrane  zzion  zzjwmc  zzzs  zzzzzz  zzzzzzz  \\\n",
       "0             0        0      0       0     0       0        0   \n",
       "1             0        0      0       0     0       0        0   \n",
       "2             0        0      0       0     0       0        0   \n",
       "3             0        0      0       0     0       0        0   \n",
       "4             0        0      0       0     0       0        0   \n",
       "...         ...      ...    ...     ...   ...     ...      ...   \n",
       "203477        0        0      0       0     0       0        0   \n",
       "203478        0        0      0       0     0       0        0   \n",
       "203479        0        0      0       0     0       0        0   \n",
       "203480        0        0      0       0     0       0        0   \n",
       "203481        0        0      0       0     0       0        0   \n",
       "\n",
       "        zzzzzzzzzzzzzzzzz  \n",
       "0                       0  \n",
       "1                       0  \n",
       "2                       0  \n",
       "3                       0  \n",
       "4                       0  \n",
       "...                   ...  \n",
       "203477                  0  \n",
       "203478                  0  \n",
       "203479                  0  \n",
       "203480                  0  \n",
       "203481                  0  \n",
       "\n",
       "[203482 rows x 86277 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv = CountVectorizer()\n",
    "doc_word = cv.fit_transform(df[\"lem\"])\n",
    "vect = pd.DataFrame(doc_word.toarray(),columns=cv.get_feature_names())\n",
    "vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:18:49.290281Z",
     "start_time": "2020-10-31T01:18:49.284928Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['zweiten', 'zweiter', 'zweitwichtigste', 'zwilling', 'zwillingen',\n",
       "       'zwingt', 'zwischen', 'zwischendurch', 'zwischenstationen', 'zyka',\n",
       "       'zynalturist', 'zynischen', 'zyoritv', 'zzcrane', 'zzion', 'zzjwmc',\n",
       "       'zzzs', 'zzzzzz', 'zzzzzzz', 'zzzzzzzzzzzzzzzzz'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.columns[-20:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:18:37.839922Z",
     "start_time": "2020-10-31T01:18:37.835755Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8532"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# previously: 94809 columns\n",
    "# spacy lem: 89679\n",
    "94809-86277"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:19:43.868460Z",
     "start_time": "2020-10-31T01:19:43.817774Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#IslamKills Are you trying to say that there w...</td>\n",
       "      <td>islamkill try say terrorist attack europe refu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Clinton: Trump should’ve apologized more, atta...</td>\n",
       "      <td>clinton trump apologize attack less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @ltapoll: Who was/is the best president of ...</td>\n",
       "      <td>good president past retweet</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @jww372: I don't have to guess your religio...</td>\n",
       "      <td>guess religion christmasaftermath</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @Shareblue: Pence and his lawyers decided w...</td>\n",
       "      <td>pence lawyer decide official email public coul...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  #IslamKills Are you trying to say that there w...   \n",
       "1  Clinton: Trump should’ve apologized more, atta...   \n",
       "2  RT @ltapoll: Who was/is the best president of ...   \n",
       "3  RT @jww372: I don't have to guess your religio...   \n",
       "4  RT @Shareblue: Pence and his lawyers decided w...   \n",
       "\n",
       "                                                 lem  \n",
       "0  islamkill try say terrorist attack europe refu...  \n",
       "1                clinton trump apologize attack less  \n",
       "2                        good president past retweet  \n",
       "3                  guess religion christmasaftermath  \n",
       "4  pence lawyer decide official email public coul...  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets = df[['text', 'lem']]\n",
    "tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-31T01:20:23.056747Z",
     "start_time": "2020-10-31T01:20:22.910466Z"
    }
   },
   "outputs": [],
   "source": [
    "with open('../data_files/tweets.pickle', 'wb') as to_write:\n",
    "   pickle.dump(tweets, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Word Clouds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### Most frequent tweeters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:55:07.792178Z",
     "start_time": "2020-10-30T16:55:07.308081Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(df['user_key'])\n",
    "\n",
    "wc = WordCloud(width=600, height=400, max_words=50).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true,
    "hidden": true
   },
   "source": [
    "### Most used hashtags in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T16:55:12.289599Z",
     "start_time": "2020-10-30T16:55:11.187562Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(df['hashtags'].apply(lambda x: \" \".join(x)))\n",
    "\n",
    "wc = WordCloud(width=800, height=600, max_words=150).generate_from_frequencies(fdist)\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Merkelmussbleiben = 'Merkel must stay'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:28.670814Z",
     "start_time": "2020-10-30T18:50:28.668347Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:29.859732Z",
     "start_time": "2020-10-30T18:50:29.855720Z"
    }
   },
   "outputs": [],
   "source": [
    "class NLPPipe:\n",
    "    \n",
    "    def __init__(self, vectorizer=CountVectorizer(), cleaning_function=None, tokenizer=None, stemmer=None):\n",
    "        '''\n",
    "        Create a pipeline that vectorizes an arbitary list of documents.\n",
    "        '''\n",
    "        self.vectorizer = vectorizer\n",
    "        self.cleaning_function = cleaning_function\n",
    "        self.tokenizer = tokenizer\n",
    "        self.stemmer = stemmer\n",
    "     \n",
    "    def clean_text(self, text):\n",
    "        text = text.astype(str)\n",
    "        cleaned_text = text.map(self.cleaning_function)\n",
    "        return cleaned_text\n",
    "    \n",
    "    def fit(self, text):\n",
    "        pass\n",
    "        \n",
    "    def transform(self, text):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:31.850267Z",
     "start_time": "2020-10-30T18:50:31.847578Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = NLPPipe(vectorizer=CountVectorizer(), \n",
    "              cleaning_function=clean_tweet, \n",
    "              tokenizer=TreebankWordTokenizer().tokenize, \n",
    "              stemmer=PorterStemmer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:50:34.586213Z",
     "start_time": "2020-10-30T18:50:34.570937Z"
    }
   },
   "outputs": [],
   "source": [
    "df2 = raw_df[['text']]\n",
    "\n",
    "tweets = df2['text']\n",
    "\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:51:26.033091Z",
     "start_time": "2020-10-30T18:51:20.765425Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "clean_tweets = nlp.clean_text(tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T18:53:44.913592Z",
     "start_time": "2020-10-30T18:53:44.907551Z"
    }
   },
   "outputs": [],
   "source": [
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T17:31:05.778478Z",
     "start_time": "2020-10-30T17:31:05.775095Z"
    }
   },
   "outputs": [],
   "source": [
    "for tweet in clean_tweets[0:5]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-30T17:31:40.518752Z",
     "start_time": "2020-10-30T17:31:40.515670Z"
    }
   },
   "outputs": [],
   "source": [
    "for tweet in raw_df.text[0:5]:\n",
    "    print(tweet, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.to_csv('data_files/clean_tweets.zip', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
